true_class_indices <- as.integer(valid_data$Digit)
n_valid <- nrow(valid_data)
# Extract the probability of the *correct* class for each observation
prob_correct_class <- fit_valid$prob[cbind(1:n_valid, true_class_indices)]
# Add small constant to avoid log(0)
prob_correct_class_safe <- pmax(prob_correct_class, 1e-15)
# Calculate Cross-Entropy (Negative Log-Likelihood)
# CE = - (1/N) * sum(log(P(y_i | x_i)))
results$valid_cross_entropy[k] <- -mean(log(prob_correct_class_safe))
}
best_k_row <- results[which.min(results$valid_cross_entropy), ]
best_k <- best_k_row$K
return(list(results_df = results, best_k = best_k))
}
#' Plot a Confusion Matrix Heatmap
#' (Helper function for Task 2)
#'
#' @param cm A confusion matrix object (from \code{table()}).
#' @param title A string for the plot subtitle.
#' @return A ggplot object.
#' @importFrom ggplot2 ggplot aes geom_tile geom_text scale_fill_gradient scale_y_discrete labs theme theme_minimal element_text
#' @importFrom rlang .data
#' @export
plot_confusion_matrix <- function(cm, title = "Confusion Matrix") {
cm_df <- as.data.frame(cm)
heatmap_plot <- ggplot(data = cm_df, aes(x = .data$Predicted, y = .data$Actual, fill = .data$Freq)) +
geom_tile(color = "black") +
geom_text(aes(label = ifelse(.data$Freq == 0, "", .data$Freq)),
color = "black", size = 4) +
scale_fill_gradient(low = "white", high = "steelblue", name = "Count") +
scale_y_discrete(limits = rev) +
labs(
title = "Confusion Matrix Heatmap",
subtitle = paste("Data:", title),
x = "Predicted Digit",
y = "Actual Digit"
) +
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
plot.subtitle = element_text(size = 14, hjust = 0.5),
axis.title = element_text(size = 14),
axis.text = element_text(size = 12, face = "bold"),
legend.position = "right"
)
return(heatmap_plot)
}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(Assignment1KNN)
library(ggplot2) # Load ggplot2 to display plots
# Load the data included in the package
data(optdigits)
# Split the data (using a reproducible seed)
datasets <- split_optdigits_data(optdigits, seed = 12345)
# Store datasets in easy-to-use variables
train_data <- datasets$train_data
valid_data <- datasets$valid_data
test_data <- datasets$test_data
# Call our function from the R/ folder
k_30_results <- evaluate_k_30(train_data, test_data)
# Print misclassification rates
cat("--- Misclassification Rates (K=30) ---\n")
cat(paste("Training Error:", round(k_30_results$error_train, 5), "\n"))
cat(paste("Test Error:    ", round(k_30_results$error_test, 5), "\n\n"))
# Plot confusion matrices
cat("--- Confusion Matrices (K=30) ---\n")
print(plot_confusion_matrix(k_30_results$cm_train, title = "Training Set (K=30)"))
print(plot_confusion_matrix(k_30_results$cm_test, title = "Test Set (K=30)"))
# This code runs the analysis for Task 4 to get 'best_k'
k_search_misclass <- find_best_k_misclassification(train_data, valid_data)
best_k_misclass <- k_search_misclass$best_k
cat(paste("Note: The 'Best K' (from Task 4) is:", best_k_misclass, "\n"))
# Call our new function from the R/ folder
examples_8 <- find_hardest_easiest_8s(train_data, best_k_misclass)
# Plot the examples
print(plot_digit(examples_8$hardest[1, ], title = "Hardest 1 (Prob '8' = 0.0)"))
print(plot_digit(examples_8$hardest[2, ], title = "Hardest 2 (Prob '8' = 0.0)"))
print(plot_digit(examples_8$hardest[3, ], title = "Hardest 3 (Prob '8' = 0.333)"))
print(plot_digit(examples_8$easiest[1, ], title = "Easiest 1 (Prob '8' = 1.0)"))
print(plot_digit(examples_8$easiest[2, ], title = "Easiest 2 (Prob '8' = 1.0)"))
# We already ran the analysis in the chunk above ('task3-find-k-first').
# Now we just plot the results.
plot_knn_errors(k_search_misclass$results_df, best_k_misclass)
# 1. Get the error rates from our 'k_search_misclass' object
train_error_best_k <- k_search_misclass$results_df$train_error[best_k_misclass]
valid_error_best_k <- k_search_misclass$results_df$valid_error[best_k_misclass]
# 2. Calculate the Test Error using the optimal K
final_model <- kknn::kknn(Digit ~ ., train_data, test_data, k = best_k_misclass, kernel = "rectangular")
pred_test <- stats::fitted(final_model)
test_error_best_k <- sum(pred_test != test_data$Digit) / nrow(test_data)
# 3. Print comparison
cat(paste("--- Error Rates for Optimal K (K=", best_k_misclass, ") ---\n"))
cat(paste("Training Error:  ", round(train_error_best_k, 5), "\n"))
cat(paste("Validation Error:", round(valid_error_best_k, 5), "\n"))
cat(paste("Test Error:      ", round(test_error_best_k, 5), "\n"))
# Call our new function from the R/ folder
k_search_ce <- find_best_k_cross_entropy(train_data, valid_data)
best_k_ce <- k_search_ce$best_k
cat(paste("The best K using Cross-Entropy is:", best_k_ce, "\n"))
# Plot the results
plot(k_search_ce$results_df$K,
k_search_ce$results_df$valid_cross_entropy,
type = "b", # "b" means both points and lines
xlab = "K (Number of Neighbors)",
ylab = "Cross-Entropy Error",
main = "Cross-Entropy Error vs. K (Validation Set)",
col = "blue",
lwd = 2)
# Add a vertical line for the best K
abline(v = best_k_ce, col = "red", lty = 2)
library(usethis)
usethis::use_mit_license(copyright_holder = "Chenzhi Ni")
devtools::document()
devtools::install()
devtools::check()
devtools::check()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# Load the package
library(Block2.RamdomForest)
# Run the simulation for Task 1
# as it takes time.
task1_results <- run_task1()
print(task1_results)
# Run the simulation for Task 2
task2_results <- run_task2()
print(task2_results)
# Run the simulation for Task 3
task3_results <- run_task3()
print(task3_results)
#' @importFrom stats runif predict var
#' @importFrom randomForest randomForest
#' @importFrom tidyr pivot_longer
NULL
#' @param n_test Size of the test dataset. Default is 1000.
#' @param condition_func A function that takes two arguments (x1, x2) and returns
#'   the binary class label (0 or 1).
#' @param nodesize The 'nodesize' parameter for randomForest.
#' @param ntree_vec A vector of 'ntree' values to test. Default is c(1, 10, 100).
#' @param seed Seed for reproducibility of test data. Default is 1234.
#'
#' @return A long-format data.frame with the results of all simulations,
#'   ready for plotting. Contains columns: `simulation_id`, `ntree`, `error_rate`.
#' @export
run_simulation <- function(n_sim = 1000, n_train = 100, n_test = 1000,
condition_func, nodesize,
ntree_vec = c(1, 10, 100), seed = 1234) {
# --- 1. Generate the fixed test dataset ---
# Set seed for reproducibility of the test data
set.seed(seed)
test_x1 <- stats::runif(n_test)
test_x2 <- stats::runif(n_test)
# Use data.frame with explicit, consistent column names
tedata <- data.frame(x1 = test_x1, x2 = test_x2)
# Apply the condition function
test_y <- condition_func(test_x1, test_x2)
telabels <- as.factor(test_y)
# Matrix to store errors from all simulations
# Each row is a simulation, each column is an ntree setting
errors_matrix <- matrix(NA, nrow = n_sim, ncol = length(ntree_vec))
colnames(errors_matrix) <- paste0("ntree_", ntree_vec)
# --- 2. Run the simulations ---
for (i in 1:n_sim) {
# Generate training data (no seed set here, so it's different each time)
train_x1 <- stats::runif(n_train)
train_x2 <- stats::runif(n_train)
# Use data.frame with the *same* explicit column names
trdata <- data.frame(x1 = train_x1, x2 = train_x2)
train_y <- condition_func(train_x1, train_x2)
trlabels <- as.factor(train_y)
# Train models for each ntree value
for (j in 1:length(ntree_vec)) {
ntree_val <- ntree_vec[j]
# Fit the random forest
set.seed(i + j)
rf_model <- randomForest::randomForest(
x = trdata,
y = trlabels,
ntree = ntree_val,
nodesize = nodesize,
keep.forest = TRUE
)
# Predict on the test data
predictions <- stats::predict(rf_model, newdata = tedata)
# Calculate misclassification error
misclassification_error <- mean(predictions != telabels)
# Store the error
errors_matrix[i, j] <- misclassification_error
}
}
# --- 3. Format and return all results ---
# Convert matrix to a long-format data.frame for easy plotting
results_df <- as.data.frame(errors_matrix)
results_df$simulation_id <- 1:n_sim
long_results <- tidyr::pivot_longer(
results_df,
cols = starts_with("ntree_"),
names_to = "ntree_setting",
names_prefix = "ntree_",
values_to = "error_rate"
)
# Convert ntree_setting back to numeric for plotting
long_results$ntree <- as.numeric(long_results$ntree_setting)
return(long_results)
}
#' Run Simulation for Task 1
#'
#' Runs the simulation experiment for Task 1 (condition: x1 < x2, nodesize: 25).
#'
#' @return A data.frame with the mean and variance of misclassification errors.
#' @export
run_task1 <- function() {
# Define the condition for Task 1
condition_task1 <- function(x1, x2) {
as.numeric(x1 < x2)
}
# Run the simulation
message("Running Task 1 (condition: x1 < x2, nodesize: 25)...")
results <- run_simulation(
condition_func = condition_task1,
nodesize = 25
)
message("Task 1 complete.")
return(results)
}
#' Run Simulation for Task 2
#'
#' Runs the simulation experiment for Task 2 (condition: x1 < 0.5, nodesize: 25).
#'
#' @return A data.frame with the mean and variance of misclassification errors.
#' @export
run_task2 <- function() {
# Define the condition for Task 2
condition_task2 <- function(x1, x2) {
as.numeric(x1 < 0.5)
}
# Run the simulation
message("Running Task 2 (condition: x1 < 0.5, nodesize: 25)...")
results <- run_simulation(
condition_func = condition_task2,
nodesize = 25
)
message("Task 2 complete.")
return(results)
}
#' Run Simulation for Task 3
#'
#' Runs the simulation experiment for Task 3
#' (condition: (x1 < 0.5 & x2 < 0.5) | (x1 > 0.5 & x2 > 0.5), nodesize: 12).
#'
#' @return A data.frame with the mean and variance of misclassification errors.
#' @export
run_task3 <- function() {
# Define the condition for Task 3
condition_task3 <- function(x1, x2) {
as.numeric((x1 < 0.5 & x2 < 0.5) | (x1 > 0.5 & x2 > 0.5))
}
# Run the simulation
message("Running Task 3 (XOR-like problem, nodesize: 12)...")
results <- run_simulation(
condition_func = condition_task3,
nodesize = 12
)
message("Task 3 complete.")
return(results)
}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# Load the package
library(Block2.RamdomForest)
# Load packages for plotting and data manipulation
library(ggplot2)
library(dplyr)
# Run the simulation for Task 1
# This now returns a long data frame with all 1000 simulation results
task1_results <- run_task1()
# 1. Calculate and print the summary table (as required by assignment)
task1_summary <- task1_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 1 ---")
print(task1_summary)
# 2. Plot the results as a boxplot
ggplot(task1_results, aes(x = factor(ntree), y = error_rate, fill = factor(ntree))) +
geom_boxplot() +
labs(
title = "Task 1: Error Rate Distribution (x1 < x2)",
x = "Number of Trees (ntree)",
y = "Misclassification Error Rate"
) +
guides(fill = "none") + # Hide the legend, it's redundant
theme_minimal()
# Run the simulation for Task 2
task2_results <- run_task2()
# 1. Calculate and print the summary table
task2_summary <- task2_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 2 ---")
print(task2_summary)
# 2. Plot the results
ggplot(task2_results, aes(x = factor(ntree), y = error_rate, fill = factor(ntree))) +
geom_boxplot() +
labs(
title = "Task 2: Error Rate Distribution (x1 < 0.5)",
x = "Number of Trees (ntree)",
y = "Misclassification Error Rate"
) +
guides(fill = "none") +
theme_minimal()
# Run the simulation for Task 3
task3_results <- run_task3()
# 1. Calculate and print the summary table
task3_summary <- task3_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 3 ---")
print(task3_summary)
# 2. Plot the results
ggplot(task3_results, aes(x = factor(ntree), y = error_rate, fill = factor(ntree))) +
geom_boxplot() +
labs(
title = "Task 3: Error Rate Distribution (XOR-like Problem)",
x = "Number of Trees (ntree)",
y = "Misclassification Error Rate"
) +
guides(fill = "none") +
theme_minimal()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# Load the package
library(Block2.RamdomForest)
# Load packages for plotting and data manipulation
library(ggplot2)
library(dplyr)
# Run the simulation for Task 1
# This now returns a long data frame with all 1000 simulation results
task1_results <- run_task1()
# 1. Calculate and print the summary table (as required by assignment)
task1_summary <- task1_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 1 ---")
print(task1_summary)
# 2. Plot the results as a bar chart (geom_col)
ggplot(task1_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
geom_col() +
# Add text labels on top of the bars
geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
labs(
title = "Task 1: Mean Error Rate (x1 < x2)",
x = "Number of Trees (ntree)",
y = "Mean Misclassification Error Rate"
) +
guides(fill = "none") + # Hide the legend
theme_minimal() +
# Give some space for the text labels
ylim(0, max(task1_summary$mean_error) * 1.1)
# Run the simulation for Task 2
task2_results <- run_task2()
# 1. Calculate and print the summary table
task2_summary <- task2_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 2 ---")
print(task2_summary)
# 2. Plot the results as a bar chart
ggplot(task2_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
geom_col() +
geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
labs(
title = "Task 2: Mean Error Rate (x1 < 0.5)",
x = "Number of Trees (ntree)",
y = "Mean Misclassification Error Rate"
) +
guides(fill = "none") +
theme_minimal() +
ylim(0, max(task2_summary$mean_error) * 1.1)
# Run the simulation for Task 3
task3_results <- run_task3()
# 1. Calculate and print the summary table
task3_summary <- task3_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 3 ---")
print(task3_summary)
# 2. Plot the results as a bar chart
ggplot(task3_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
geom_col() +
geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
labs(
title = "Task 3: Mean Error Rate (XOR-like Problem)",
x = "Number of Trees (ntree)",
y = "Mean Misclassification Error Rate"
) +
guides(fill = "none") +
theme_minimal() +
ylim(0, max(task3_summary$mean_error) * 1.1)
devtools::check()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# Load the package
library(Block2.RamdomForest)
# Load packages for plotting and data manipulation
library(ggplot2)
library(dplyr)
# Run the simulation for Task 1
# This now returns a long data frame with all 1000 simulation results
task1_results <- run_task1()
# 1. Calculate and print the summary table (as required by assignment)
task1_summary <- task1_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 1 ---")
print(task1_summary)
# 2. Plot the results as a bar chart (geom_col)
ggplot(task1_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
geom_col() +
# Add text labels on top of the bars
geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
labs(
title = "Task 1: Mean Error Rate (x1 < x2)",
x = "Number of Trees (ntree)",
y = "Mean Misclassification Error Rate"
) +
guides(fill = "none") + # Hide the legend
theme_minimal() +
# Give some space for the text labels
ylim(0, max(task1_summary$mean_error) * 1.1)
# Run the simulation for Task 2
task2_results <- run_task2()
# 1. Calculate and print the summary table
task2_summary <- task2_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 2 ---")
print(task2_summary)
# 2. Plot the results as a bar chart
ggplot(task2_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
geom_col() +
geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
labs(
title = "Task 2: Mean Error Rate (x1 < 0.5)",
x = "Number of Trees (ntree)",
y = "Mean Misclassification Error Rate"
) +
guides(fill = "none") +
theme_minimal() +
ylim(0, max(task2_summary$mean_error) * 1.1)
# Run the simulation for Task 3
task3_results <- run_task3()
# 1. Calculate and print the summary table
task3_summary <- task3_results %>%
group_by(ntree) %>%
summarise(
mean_error = mean(error_rate),
variance_error = var(error_rate)
)
print("--- Summary Statistics for Task 3 ---")
print(task3_summary)
# 2. Plot the results as a bar chart
ggplot(task3_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
geom_col() +
geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
labs(
title = "Task 3: Mean Error Rate (XOR-like Problem)",
x = "Number of Trees (ntree)",
y = "Mean Misclassification Error Rate"
) +
guides(fill = "none") +
theme_minimal() +
ylim(0, max(task3_summary$mean_error) * 1.1)
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::check()

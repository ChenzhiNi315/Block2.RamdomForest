---
title: "Assignment 1: Random Forest Simulations"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assignment 1: Random Forest Simulations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
# Load the package
library(Block2.RamdomForest)
# Load packages for plotting and data manipulation
library(ggplot2)
library(dplyr)
```
 
# Introduction

This vignette runs the random forest simulations as described in Assignment 1. We will compare the performance of random forests with different numbers of trees (1, 10, 100) on three different classification tasks.

We will run 1000 simulations, each time using a training set of size 100, and evaluate the model on a fixed test set of size 1000. We will report the mean and variance of the misclassification error.

Note: Running 1000 simulations may take a few minutes.

## Task 1: x1 < x2

Condition: `y = as.numeric(x1 < x2)` Nodesize: 25

```{r task1}
# Run the simulation for Task 1
# This now returns a long data frame with all 1000 simulation results
task1_results <- run_task1()

# 1. Calculate and print the summary table (as required by assignment)
task1_summary <- task1_results %>%
  group_by(ntree) %>%
  summarise(
    mean_error = mean(error_rate),
    variance_error = var(error_rate)
  )

print("--- Summary Statistics for Task 1 ---")
print(task1_summary)
```
Task 1 Plot:
```{r task1.plot, cache=TRUE, fig.width=12, out.width="100%"}
# 2. Plot the results as a bar chart (geom_col)
ggplot(task1_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
  geom_col() +
  # Add text labels on top of the bars
  geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
  labs(
    title = "Task 1: Mean Error Rate (x1 < x2)",
    x = "Number of Trees (ntree)",
    y = "Mean Misclassification Error Rate"
  ) +
  guides(fill = "none") + # Hide the legend
  theme_minimal() +
  # Give some space for the text labels
  ylim(0, max(task1_summary$mean_error) * 1.1)
```

### Task 1 Results Analysis: As shown in the table above, the mean error rate drops significantly as the number of trees increases from 1 to 10. It drops slightly further when increasing from 10 to 100 trees. The variance of the error also decreases as the number of trees grows, indicating that the model's stability is improving.

## Task 2: x1 < 0.5

Condition: `y = as.numeric(x1 < 0.5)` Nodesize: 25

```{r task2, cache=TRUE, fig.width=12, out.width="100%"}
# Run the simulation for Task 2
task2_results <- run_task2()

# 1. Calculate and print the summary table
task2_summary <- task2_results %>%
  group_by(ntree) %>%
  summarise(
    mean_error = mean(error_rate),
    variance_error = var(error_rate)
  )

print("--- Summary Statistics for Task 2 ---")
print(task2_summary)
```
```{r task2.plot, cache=TRUE, fig.width=12, out.width="100%"}
# 2. Plot the results as a bar chart
ggplot(task2_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
  geom_col() +
  geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
  labs(
    title = "Task 2: Mean Error Rate (x1 < 0.5)",
    x = "Number of Trees (ntree)",
    y = "Mean Misclassification Error Rate"
  ) +
  guides(fill = "none") +
  theme_minimal() +
  ylim(0, max(task2_summary$mean_error) * 1.1)
```


### Task 2 Results Analysis: This is a much simpler, linearly separable problem (the decision boundary is `x1 = 0.5`).

- `ntree = 1`: A single decision tree (which is a "stump" or very shallow tree due to `nodesize = 25`) already performs exceptionally well, with a very low error rate.

- `ntree = 10` and `ntree = 100`: Increasing the number of trees reduces the error rate further, to nearly zero. Because the problem is so simple, the aggregation effect of the random forest only slightly improves upon an already excellent performance.

## Task 3: XOR-like Problem

Condition: `y = as.numeric((x1 < 0.5 & x2 < 0.5) | (x1 > 0.5 & x2 > 0.5))` Nodesize: 12

This condition represents an "XOR" type problem, where the class is 1 in the bottom-left and top-right quadrants, and 0 otherwise. This is a non-linearly separable problem.

```{r task3, cache=TRUE, fig.width=12, out.width="100%"}
# Run the simulation for Task 3
task3_results <- run_task3()

# 1. Calculate and print the summary table
task3_summary <- task3_results %>%
  group_by(ntree) %>%
  summarise(
    mean_error = mean(error_rate),
    variance_error = var(error_rate)
  )

print("--- Summary Statistics for Task 3 ---")
print(task3_summary)
```
```{r task3.plot, cache=TRUE, fig.width=12, out.width="100%"}
# 2. Plot the results as a bar chart
ggplot(task3_summary, aes(x = factor(ntree), y = mean_error, fill = factor(ntree))) +
  geom_col() +
  geom_text(aes(label = round(mean_error, 4)), vjust = -0.5) +
  labs(
    title = "Task 3: Mean Error Rate (XOR-like Problem)",
    x = "Number of Trees (ntree)",
    y = "Mean Misclassification Error Rate"
  ) +
  guides(fill = "none") +
  theme_minimal() +
  ylim(0, max(task3_summary$mean_error) * 1.1)
```

### Task 3 Results Analysis:
`ntree = `1: A single decision tree (even with `nodesize = 12`, which is deeper than 25) struggles to capture this non-linear XOR relationship, resulting in a relatively high mean error rate (around 0.25, which is better than random guessing at 0.5).

`ntree = 10` and `ntree = 100`: The mean error rate drops dramatically as the number of trees increases. The forest with 100 trees performs extremely well, with a very low error rate. This demonstrates the power of random forests in handling non-linear problems.

## Task 4: Oral Defense Questions

### a. What happens with the mean error rate when the number of trees in the random forest grows? Why?

Answer:

What happens: As seen in the plots for Task 1 and Task 3, as the number of trees (ntree) grows, the mean error rate decreases, and the variance of the error rate (the height of the boxplots) also decreases dramatically. The error rate eventually converges to a stable, low value.

Why: This is due to Variance Reduction, a key benefit of bagging and random forests.

A single decision tree (`ntree = 1`) has low bias (it can fit complex data) but high variance (it's very sensitive to the specific training data). This is visible in the Task 1 and 3 plots, where the `ntree = 1` box is very tall and has many outliers.

By building many trees on different subsets of data and averaging their (vote) predictions, the random forest cancels out the individual errors. The errors of one tree are (ideally) uncorrelated with the errors of another.

By the Law of Large Numbers, averaging these uncorrelated predictions significantly reduces the variance of the overall ensemble. Adding more trees continues to reduce this variance, leading to a more stable and accurate model, until the performance converges.


In summary, adding more trees reduces the model's variance, making its predictions more robust and accurate, until the performance converges.

### b. The third dataset represents a slightly more complicated classification problem than the first one. Still, you should get better performance for it when using sufficient trees in the random forest. Explain why you get better performance.
Answer:

Problem Analysis:

Dataset 1 (Task 1): `x1 < x2`. This is a linearly separable problem. The decision boundary is the diagonal line `x1 = x2`.

Dataset 3 (Task 3): `(x1 < 0.5 & x2 < 0.5) | (x1 > 0.5 & x2 > 0.5)`. This is an "XOR" problem. The decision boundaries are rectangular, formed by the lines `x1 = 0.5` and `x2 = 0.5`. This is a non-linear problem.

Why does Task 3 get better performance?

(Note: The premise of this question is slightly counter-intuitive, as XOR is often considered "harder" than a simple linear problem. The key here is the nature of the decision boundary and how decision trees work.)

Decision trees (the base learners for random forests) work by making axis-aligned splits.

- For Task 3 (XOR Problem): The decision boundaries are `x1 = 0.5` and `x2 = 0.5`. These boundaries are perfectly aligned with the splitting mechanism of a decision tree. A single (sufficiently deep) decision tree can learn this "XOR" rule perfectly by splitting once at x1 = 0.5 and again at x2 = 0.5. Therefore, this problem is "easy" for a tree-based method. The nodesize = 12 allows the trees to be deep enough to easily find these two splits.

- For Task 1 (Linear Problem): The decision boundary is `x1 = x2`. This is a diagonal line. A decision tree cannot create a diagonal split. To approximate this diagonal line, the tree must make many axis-aligned splits, creating a "staircase" approximation.

This approximation will never be perfect. The `nodesize = 25` forces the trees to be shallower, making it even harder for them to create a fine-grained "staircase" to approximate the diagonal. This introduces an inherent model bias.

Conclusion: Even though Task 3 is conceptually more complex (non-linear) than Task 1 (linear), Task 3 is actually an easier problem for a random forest (a tree-based method).

The decision boundaries of Task 3 (`x1 = 0.5`, `x2 = 0.5`) are a perfect fit for the trees' natural (axis-aligned) splitting. The trees in the forest can therefore learn this rule with very high accuracy and low bias.

In contrast, the diagonal boundary of Task 1 (`x1 = x2`) is "hard" for a decision tree. Each tree can only approximate it with a staircase, which introduces a fundamental model bias.

Therefore, when enough trees are used to reduce the variance, the random forest for Task 3 (which has low bias) will outperform the random forest for Task 1 (which has a higher bias due to the diagonal approximation). This is why we see a lower final mean error for Task 3 (`ntree=100`, error ~0.002) than for Task 1 (`ntree=100`, error ~0.027) in our simulations.
